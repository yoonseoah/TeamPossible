<!-- Template for PROJECT REPORT of CapstoneDesign 2025-2H, initially written by khyoo -->
<!-- 본 파일은 2025년도 컴공 졸업프로젝트의 <1차보고서> 작성을 위한 기본 양식입니다. -->
<!-- 아래에 "*"..."*" 표시는 italic체로 출력하기 위해서 사용한 것입니다. -->
<!-- "내용"에 해당하는 부분을 지우고, 여러분 과제의 내용을 작성해 주세요. -->

# Team-Info
| (1) 과제명 | *Generating Missing Auditory Modality via Semantic Mapping for Video Action Recognition*
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | *21-가능한* |
| (3) 팀 구성원 | 김지은 (2140010): 리더, *기본 데이터 전처리, caption-based dictionary 활용 아키텍처 설계 및 구현, transformer 활용 오디오 특징 추출* <br> 윤서아 (2168019): 팀원, *TSN 기반 feature extraction, BERT 활용 semantic mapping dictionary 설계* <br> 장은성 (2271052) : 팀원, *transformer 기반 프레임워크 중 video 전처리, feature 추출 부분 설계 및 구현*            |
| (4) 팀 지도교수 | 이형준 교수님 |
| (5) 과제 분류 | *연구 과제* |
| (6) 과제 키워드 | *Generative AI, Video Action Recognition, Multimodal, Semantic Mapping*  |
| (7) 과제 내용 요약 | *본 연구는 비디오 시퀀스를 기반으로 오디오 피처를 재구성하여, 결손된 오디오 데이터를 보완하고 이를 통해 모델의 정확도를 향상시키는 것을 목표로 한다. 이를 위해 비디오와 오디오 간의 의미적 정렬을 수행하는 개념인 Semantic Mapping을 차용하였다. 두 가지 아키텍처를 제안하는데, 공통적으로 비디오로부터 오디오 피처를 생성하지만, 검증 및 활용 방식에서 차이를 보인다. 첫 번째 아키텍처는 CNN과 LSTM을 활용하여 오디오 피처를 생성한 후, Semantic Dictionary를 이용해 의미적 유효성을 검증한다. 이 검증을 통과한 피처만 학습에 활용하는 방식이다. 두 번째 아키텍처는 Transformer를 활용해 대표 프레임을 추출하고, 캡션으로 변환한 뒤 이를 키로 사용하는 Semantic Dictionary을 만든다. 이 사전을 참조하여 오디오 피쳐를 생성해낸다. 두 아키텍처를 비교 분석함으로써, 의미 기반 피처 정렬 방식이 오디오 생성 및 영상 이해 성능에 미치는 영향을 규명하고자 한다.* |

<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 | 비디오 기반 행동 인식에서는 멀티모달(시각+청각) 정보를 활용한 학습이 정밀도를 높이는 데 필수적이다. 하지만 현실에서는 오디오가 손실되거나, 특정 모달리티만 라벨링된 데이터셋이 많아 멀티모달 학습의 효과가 제한된다. 특히 오디오 모달리티는 파일 손상, 녹음 품질 저하, 무관한 배경음 등으로 인해 활용이 어려운 경우가 많다. Target Customer는 멀티모달 행동 인식 시스템을 개발하려는 연구자, 기업, 산업계로, 오디오 결손 상황에서 학습 정확도를 유지할 수 있는 보완적 해결책이 필요하다. |
| (2) 기존연구와의 비교 | 기존 연구는 문제 해결을 위해 비디오의 시공간적 특징을 매핑해 오디오 피처를 재구성하는데, 이는 비디오 시퀀스와 의미적 정합성을 고려하지 못한다는 한계를 가진다. 또한, 비디오-오디오 의미적 매핑 사전을 구성해 비디오와 무관한 데이터를 드롭아웃하는 기법을 적용한 기존 연구는 여전히 오디오가 결손된 상황에 대한 극복이 어렵다. 본 과제는 이들 연구와 달리, 비디오-오디오 간의 **의미적 정합성(semantic consistency)**을 고려하여 오디오 피처를 생성하고, 정합성 검증을 통해 학습에 사용함으로써 더 높은 신뢰성과 정확도를 보장한다는 점에서 차별화된다. |
| (3) 제안 내용 | 본 프로젝트는 결손된 오디오 피처를 보완하고 행동 인식의 정확도를 향상시키기 위한 두 가지 아키텍처를 제안한다.<br> **① Semantic Validation Architecture**: Transformer 기반으로 비디오에서 피처를 추출한 후, 생성된 오디오 피처에 대해 액션 라벨 예측을 수행하고, 시맨틱 사전을 이용하여 정합성을 검증한 뒤, 정합성이 확인된 피처만 학습에 사용.<br> **② Caption-based Attention Mapping Architecture**: Transformer 기반으로 대표 RGB 프레임에서 자연어 캡션을 생성하고, 이를 시맨틱 사전의 키로 활용하여 비디오와 오디오 라벨 간의 의미 매핑을 attention 기반으로 정교화하여 정합성을 강화. |
| (4) 기대효과 및 의의 | - 오디오 결손 상황에서도 높은 정확도를 유지하는 행동 인식 모델 구현 가능<br> - 의미 기반 피처 생성 및 검증을 통해 멀티모달 학습의 신뢰성 제고<br> - 행동 인식 분야를 넘어 결손 오디오 복원, 의료 영상 등 다양한 멀티모달 응용 분야로 확장 가능<br> - AGI(Artificial General Intelligence)를 위한 인간 유사 인지 능력 구현에 기여 |
| (5) 주요 기능 리스트 | - **비디오 기반 오디오 피처 생성 모듈**: Transformer를 이용해 시공간 정보를 인코딩하고 오디오 피처 생성<br> - **시맨틱 사전 구축 및 활용 기능**: 비디오-오디오 간 의미적 일치 여부를 평가할 수 있도록 액션 라벨, 캡션 등을 기반으로 의미 사전 구성<br> - **오디오 피처 정합성 검증 기능**: 생성된 오디오 피처의 의미적 유효성을 판별하여, 학습에 사용 가능한 피처를 선별<br> - **자연어 캡셔닝 기반 매핑 강화 기능**: 대표 프레임을 텍스트로 표현하여 의미 정합성을 강화하고 매핑 정확도를 향상<br> - **모델 비교 평가 시스템**: 두 가지 제안 아키텍처의 학습 정확도, 정합성 판단 성능 등을 비교 분석하는 실험 기능 포함 |


<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 | *프로젝트를 완성하기 위해 필요한 요구사항을 설명하기에 가장 적합한 방법을 선택하여 기술* <br> 예) <br> - 기능별 상세 요구사항(또는 유스케이스) <br> - 설계 모델(클래스 다이어그램, 클래스 및 모듈 명세서) <br> - UI 분석/설계 모델 <br> - E-R 다이어그램/DB 설계 모델(테이블 구조) |
| (2) 전체 시스템 구성 | *프로젝트를 위하여, SW 전체 시스템의 구조를 보인다. (가능하다면, 사용자도 포함) <br> 주요 SW 모듈을 보이고, 각각의 역할을 기술한다. <br>만약, 오픈소스 혹은 외부 모듈을 사용한다면 이또한 기술한다.* |
| (3) 주요엔진 및 기능 설계 | *프로젝트의 주요 기능 혹은 모듈의 설계내용에 대하여 기술한다 <br> SW 구조 그림에 있는 각 Module의 상세 구현내용을 자세히 기술한다.* |
| (4) 주요 기능의 구현 | *<주요기능리스트>에 정의된 기능 중 최소 2개 이상에 대한 상세 구현내용을 기술한다.* |
| (5) 기타 | *기타 사항을 기술*  |

<br>
