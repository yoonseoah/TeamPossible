# Team-Info
| (1) 과제명 | Generating Missing Auditory Modality via Semantic Mapping for Video Action Recognition
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | 21-가능한 |
| (3) 팀 구성원 | **김지은** (2140010): 리더, *기본 데이터 전처리, caption-based dictionary 활용 아키텍처 설계 및 구현, transformer 활용 오디오 특징 추출* <br> **윤서아** (2168019): 팀원, *TSN 기반 feature extraction, BERT 활용 semantic mapping dictionary 설계* <br> **장은성** (2271052) : 팀원, *transformer 기반 프레임워크 중 video 전처리, feature 추출 부분 설계 및 구현*            |
| (4) 팀 지도교수 | 이형준 교수님 |
| (5) 과제 분류 | 연구 과제 |
| (6) 과제 키워드 | Generative AI, Video Action Recognition, Multimodal, Semantic Mapping  |
| (7) 과제 내용 요약 | 본 연구는 비디오 시퀀스를 기반으로 오디오 피처를 재구성하여, 결손된 오디오 데이터를 보완하고 이를 통해 모델의 정확도를 향상시키는 것을 목표로 한다. 이를 위해 비디오와 오디오 간의 의미적 정렬을 수행하는 개념인 Semantic Mapping을 차용하였다. 두 가지 아키텍처를 제안하는데, 공통적으로 비디오로부터 오디오 피처를 생성하지만, 검증 및 활용 방식에서 차이를 보인다. 첫 번째 아키텍처는 CNN과 LSTM을 활용하여 오디오 피처를 생성한 후, Semantic Dictionary를 이용해 의미적 유효성을 검증한다. 이 검증을 통과한 피처만 학습에 활용하는 방식이다. 두 번째 아키텍처는 Transformer를 활용해 대표 프레임을 추출하고, 캡션으로 변환한 뒤 이를 키로 사용하는 Semantic Dictionary을 만든다. 이 사전을 참조하여 오디오 피쳐를 생성해낸다. 두 아키텍처를 비교 분석함으로써, 의미 기반 피처 정렬 방식이 오디오 생성 및 영상 이해 성능에 미치는 영향을 규명하고자 한다. |

<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 | 비디오 기반 행동 인식에서는 멀티모달(시각+청각) 정보를 활용한 학습이 정밀도를 높이는 데 필수적이다. 하지만 현실에서는 오디오가 손실되거나, 특정 모달리티만 라벨링된 데이터셋이 많아 멀티모달 학습의 효과가 제한된다. 특히 오디오 모달리티는 파일 손상, 녹음 품질 저하, 무관한 배경음 등으로 인해 활용이 어려운 경우가 많다. Target Customer는 멀티모달 행동 인식 시스템을 개발하려는 연구자, 기업, 산업계로, 오디오 결손 상황에서 학습 정확도를 유지할 수 있는 보완적 해결책이 필요하다. |
| (2) 기존연구와의 비교 | 기존 연구는 문제 해결을 위해 비디오의 시공간적 특징을 매핑해 오디오 피처를 재구성하는데, 이는 비디오 시퀀스와 의미적 정합성을 고려하지 못한다는 한계를 가진다. 또한, 비디오-오디오 의미적 매핑 사전을 구성해 비디오와 무관한 데이터를 드롭아웃하는 기법을 적용한 기존 연구는 여전히 오디오가 결손된 상황에 대한 극복이 어렵다. 본 과제는 이들 연구와 달리, 비디오-오디오 간의 **의미적 정합성(semantic consistency)**을 고려하여 오디오 피처를 생성하고, 정합성 검증을 통해 학습에 사용함으로써 더 높은 신뢰성과 정확도를 보장한다는 점에서 차별화된다. |
| (3) 제안 내용 | 본 프로젝트는 결손된 오디오 피처를 보완하고 행동 인식의 정확도를 향상시키기 위한 두 가지 아키텍처를 제안한다. <br> <br> **① Semantic Validation Architecture**: Transformer 기반으로 비디오에서 피처를 추출한 후, 생성된 오디오 피처에 대해 액션 라벨 예측을 수행하고, 시맨틱 사전을 이용하여 정합성을 검증한 뒤, 정합성이 확인된 피처만 학습에 사용.<br> <br> **② Caption-based Attention Mapping Architecture**: Transformer 기반으로 대표 RGB 프레임에서 자연어 캡션을 생성하고, 이를 시맨틱 사전의 키로 활용하여 비디오와 오디오 라벨 간의 의미 매핑을 attention 기반으로 정교화하여 정합성을 강화. |
| (4) 기대효과 및 의의 | - 오디오 결손 상황에서도 높은 정확도를 유지하는 행동 인식 모델 구현 가능<br> - 의미 기반 피처 생성 및 검증을 통해 멀티모달 학습의 신뢰성 제고<br> - 행동 인식 분야를 넘어 결손 오디오 복원, 의료 영상 등 다양한 멀티모달 응용 분야로 확장 가능<br> - AGI(Artificial General Intelligence)를 위한 인간 유사 인지 능력 구현에 기여 |
| (5) 주요 기능 리스트 | **[공통 모듈]**<br> - Feature Extractor<br> - Feature Fusion & Classification<br><br>**[첫번째 아키텍처 모듈]**<br> - Semantic Dictionary<br> - Audio Feature Generator<br> - Multi Label Predictor<br> - Feature Filter<br><br>**[두번째 아키텍처 모듈]**<br> - Representative Frame Extractor & CLIP-based Caption Generator<br> - Semantic Dictionary<br> - Dictionary Referencing<br> - Audio Feature Generator | |


<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 | **[기능별 상세 요구사항]** <br> - 비디오의 시각적 정보와 시퀀스 정보로부터 오디오 피처를 생성한다. <br> - 생성된 오디오 피처의 의미적 정합성을 검증한다 <br> - 또는, 피처 자체에 정합성 부여를 위해 딕셔너리를 활용한다 <br> - 의미 정합성이 부여된 오디오 피처만 비디오 피처와 fuse하여 모델 학습에 사용한다. <br> - 두 아키텍처의 성능을 비교/분석해 최적화한다. <br> <br> **[설계 모델]** <br> - video feature extractor, audio feature extractor, audio feature generator, audio-video semantic dictionary, trainer, evaluator <br> - 위와 같이 모듈을 크게 6개로 나누어 설계했으며 아키텍처에 따라 세부 구조는 상이하다. <br> <br> **[데이터 셋]** <br> - 비디오: Moments in Time(MiT) <br> - 오디오: AudioSet |
| (2) 전체 시스템 구성 | **[데이터 전처리]** <br> MiT(Moments in Time) 데이터셋 사용 (각 비디오는 3초, 비디오에 따라 오차 있으며, RGB 비디오, action calss는 304개).  각 비디오를 선행논문에 따라 6fps의 RGB frames로 추출했다.  <br> 6fps로 추출 시 비디오 당  15frames에서 24frames 정도로 오차가 발생, 그 중 18 frames인 비디오가 가장 개수가 많다. <br> → 따라서 초기 모델 설계 시 안정성을 위해 가장 개수가 많은 18frames 비디오로 학습 및 튜닝 진행하기로 결정했다. <br> 또한 클래스 당 비디오가 1000개~4000개로, 데이터의 크기가 매우 크다. <br> → 전체 클래스 304개 중 랜덤한 20개로만 실험을 진행하기로 결정했다. <br> <br> **[첫번째 아키텍처]** <br> ![Architecture 1](./images/arc1.png) <br> - 이미지와 오디오 피처는 transformer를 사용해 임베딩으로 처리한다. <br> - LSTM을 기반으로 오디오 피처를 생성한다. <br> - 생성한 오디오 피처를 사전 학습된 AST classifier를 사용해 multi label prediction 한다. <br> - 예측된 라벨과 Semantic Dictionary의 유사도 비교를 통해 의미 반영 정도를 계산한다. <br> - Threshold N을 지정해 N을 넘은 유사도를 가진 피처만을 '유의미한 피처'로 필터링해 학습에 활용한다. (이 때, N은 최적화 되어야 한다.) <br> - 이 때, Semantic Dictionary는 MiT의 action 라벨과 AudioSet의 오디오 라벨 간의 의미적 관계를 BERT로 매핑한 사전이다. <br> - 필터링된 오디오 피처와 비디오 피처를 fuse하여 최종 classification에 사용한다. <br> <br> **[두번째 아키텍처]** <br> ![Architecture 1](./images/arc2.png) <br> - 이미지와 오디오 피처는 transformer를 사용해 임베딩으로 처리한다. <br> - 프레임 간 object의 이동량을 기준으로 영상 당 n개의 대표 이미지를 계산한다. <br> - n개의 대표 이미지에 대해 CLIP 모델을 사용해 캡션을 생성한다. <br> - N개의 캡션과 유사한 키 캡션을 Semantic Dictionary에서 찾는다. <br> - 키 캡션에 매핑된 오디오 라벨 임베딩을 참조한다. <br> - 이 때, Semantic Dictionary는 학습 비디오의 대표 이미지 캡션과 사전 학습된 AST classifier로 추출한 multi 오디오 라벨 간의 의미적 관계를 BERT로 매핑한 사전이다. <br> - 랜덤 노이즈를 초기 input으로 하는 GAN 구조로 오디오 피처를 생성한다. - 이 때, Dynamic Time Warping을 이용해 비디오와 오디오 피처의 시퀀스를 정합하게 한다. <br> - 원본 비디오의 오디오 피처를 ground truth로 하여 생성 네트워크를 학습시킨다. <br> - 생성된 오디오 피처와 비디오 피처를 fuse하여 최종 classification에 사용한다. |
| (3) 주요엔진 및 기능 설계 | **[공통 모듈]** <br> **RGB Frames Extractor**: 이 모듈은 모델 학습을 위한 비디오 전처리 도구로, cv2, os, csv, shutil, ThreadPoolExecutor 등의 라이브러리를 활용하여 원본 비디오에서 6fps 간격으로 RGB 프레임을 추출하고, 18프레임을 정확히 가진 비디오만 필터링하여 저장한다. 추출된 프레임은 프레임 개수를 분기로 18frames 또는 others 폴더에 저장되며, 처리 여부는 CSV로 기록되어 중복을 방지한다. 전체 클래스별 비디오를 병렬 처리하여 속도를 높였다. <br> <br> **Optical Flow Frames Extractor**: 이 모듈은 RGB 프레임 간의 움직임 정보를 시각화하기 위해 Optical Flow 이미지를 생성하는 전처리 도구다. cv2, os, numpy, ProcessPoolExecutor, ThreadPoolExecutor 등의 라이브러리를 활용해 프레임 간의 흐름을 Farneback 알고리즘으로 계산하고, 이를 HSV → BGR 포맷의 컬러 이미지로 변환하여 저장한다. 비디오당 총 18개의 Optical Flow 이미지를 생성하며, 첫 프레임은 복제하여 정확히 18개로 맞춘다. 클래스별 학습/검증 비디오를 병렬로 처리하여 효율성을 높이고, 이미 처리된 비디오는 건너뛰도록 설계되었다. <br> <br> **Audio Extractor**: 이 모듈은 RGB 프레임 수가 18개로 필터링된 비디오에 대해서만 오디오를 추출하고, 그 중 무음이 아닌 오디오만 선별하여 학습에 사용할 수 있도록 한다. subprocess, wave, numpy, ThreadPoolExecutor 등의 라이브러리를 활용해 먼저 모든 비디오에서 .wav 형식의 오디오를 추출하며, ffmpeg를 통해 16kHz, mono, 16bit PCM 설정으로 변환한다. 이후 wave 모듈과 RMS 계산을 통해 사운드의 데시벨 기준으로 무음 여부를 판단하고, 무음으로 판단된 오디오는 자동으로 삭제되며, 해당 비디오 ID는 별도의 CSV에 기록되어 중복 처리를 방지한다. 전체 처리 과정은 병렬로 수행된다. <br> 이렇게 추출된 .wav파일을 AST 모델을 사용해 특징 추출하였다. 먼저 오디오를 spectrogram으로 변환한 후, 이를 패치로 나누고 positional embedding 및 [CLS] 토큰을 맨 앞에 붙여 transformer의 인풋으로 사용하였다. 주로 피처는 [CLS] 토큰의 output을 사용하지만, 본 연구에서는 오디오 및 비디오를 시퀀스로 처리하는 것이 핵심이기 때문에, [CLS] 토큰이 아닌 그 뒤에 붙는 patch sequence의 transformer output을 피처로 사용하였다. <br> <br> **ViT-Based Video Feature Extractor**: 이 모듈은 RGB 및 Optical Flow 이미지 프레임으로부터 ViT (Vision Transformer) 기반 구조를 활용해 비디오 피처를 추출한다. 각 프레임은 spatial patch 단위로 임베딩되며, 시간 정보를 포함한 spatiotemporal token으로 확장된다. 이후 Hugging Face의 ViT 모델을 활용하여, Transformer 인코더를 통해 비디오 피처를 추출한다. ViT 내부의 patch embedding 레이어는 별도의 전처리 과정을 거친 입력을 그대로 활용하기 때문에 nn.Identity()로 대체하였다. 또한, 일반적인 ViT가 [CLS] 토큰만을 출력으로 사용하는 것과 달리, 본 구조에서는 각 patch sequence의 Transformer 출력을 활용하여 최종적으로 [18, 768] 형태의 시퀀스 기반 비디오 피처를 추출한다. 이러한 과정을 RGB 및 Flow 모달리티 각각에 대해 수행하며, 결과는 클래스명과 비디오 ID를 키로 하는 딕셔너리 형태로 구성하여 .npy 파일로 저장한다. <br> <br> **AST-Based Audio Feature Extractor**: 이 모듈은 추출된 .wav 오디오 파일로부터 AST (Audio Spectrogram Transformer) 모델을 활용하여 시퀀스 기반 오디오 피처를 추출한다. 먼저 오디오를 18개의 동일 길이 waveform segment로 분할한 후, 각 segment를 Mel Spectrogram으로 변환하고 AST 모델에 입력한다. 일반적인 방식과 달리 [CLS] 토큰의 출력은 사용하지 않고, 각 segment에 대응하는 patch sequence의 Transformer 출력값만을 피처로 사용한다. 이렇게 얻은 [18, 768] 차원의 시퀀스 피처는 별도의 linear projection을 거쳐 [18, 128] 차원으로 축소되며, 이는 오디오 시계열로서 저장된다. 최종적으로 클래스명과 비디오 ID를 키로 하는 딕셔너리 형태로 모든 오디오 피처를 구성하고 .npy 파일로 저장한다. <br> <br> **[첫번째 아키텍처 모듈]** <br> **Multi Label Predictor**: 생성된 오디오 피처에 대해 오디오 라벨을 예측하기 위해, 사전학습된 AST(Audio Spectrogram Transformer)를 사용하였다. <br> **→ 기능 설계**: 오디오 피처를 log-Mel spectrogram 형태로 재구성한 뒤 AST 모델을 통해 multi-label classification을 수행하도록 하였고, 이 과정에서 예측된 라벨은 후속 Semantic Matching에 활용된다. 해당 모듈은 AudioSet의 멀티 라벨 구조를 반영하며, 예측 결과는 각 프레임 단위가 아닌 전체 시퀀스를 통합한 embedding 기반 라벨 벡터로 요약된다. <br> **→ 실험 내용 및 결과**: 현재는 LSTM 기반으로 생성된 오디오 피처를 입력으로 하여, 이를 AST에 통과시켜 예측된 라벨 분포가 원래 ground truth 오디오로 예측된 라벨과 유사한지를 비교하였다. 시각적으로는 라벨 벡터 간 cosine similarity를 활용하였고, 예측 라벨의 평균 precision은 약 0.41, recall은 0.36 수준으로 측정되었다. 이는 의미적으로 근접한 오디오 라벨 분포를 생성해냈음을 보여준다. <br> <br> **Semantic Dictionary**: MiT의 action 라벨과 AudioSet의 오디오 라벨 간의 의미적 관계를 BERT 임베딩을 통해 사전으로 구축한다. <br> **→ 기능 설계**:: MiT 기반 action 라벨들과 AudioSet 오디오 라벨 간의 의미적 유사도를 BERT 임베딩으로 계산하여 사전화하였고, 이 때 각 라벨은 BERT tokenizer를 거쳐 sentence embedding으로 정규화되었다. Dictionary는 action_label을 key로, 해당 label과 의미적으로 가장 가까운 오디오 라벨 embedding들과 그 유사도를 포함한 구조로 저장된다. 이 모듈은 후속 filtering 단계에서 의미적 일관성을 검증하는 근거로 활용된다. <br> **→ 실험 내용 및 결과**: 실험적으로 각 action label에 대해 dictionary 내 가장 유사한 오디오 라벨을 top-5 기준으로 평가한 결과, semantic relevance 기준 상위 5개 라벨 중 실제 ground truth에 포함된 라벨이 평균 3.7개 포함되는 것으로 나타났다. 이는 사전 구축된 딕셔너리가 실제 의미 기반 라벨 매핑을 잘 반영하고 있음을 보여주며, filtering 기준으로 사용하기에 충분한 정밀도를 확보했음을 의미한다.<br> <br> **Feature Filter**: 생성된 피처에서 예측한 multi label과 semantic dictionary를 비교, threshold를 기준으로 생성 피처를 필터링한다. <br> **→ 기능 설계**: 생성된 오디오 피처의 semantic validity를 판단하기 위해 Feature Filter 모듈을 도입하였다. 즉, 오디오 피처의 정합성 확보를 위하여, Multi Label Predictor를 통해 예측된 라벨 임베딩과 Semantic Dictionary 내 action 라벨 임베딩 간 cosine similarity를 계산하고, 해당 유사도가 사전 정의된 threshold N 이상인 경우에만 해당 피처를 retain하는 방식으로 Filtering Engine을 구현하였다. 이 threshold는 실험적으로 설정되며, precision-recall tradeoff 조절을 위한 핵심 하이퍼파라미터로 기능한다. 필터링된 오디오 피처만이 최종 classification에 사용되므로, 이 모듈은 전체 아키텍처의 정합성 및 분류 성능 향상에 결정적인 역할을 한다. <br> **→ 실험 내용 및 결과**: Filtering threshold를 N=0.72로 설정한 실험에서, 전체 생성 피처 중 약 62.3%가 retain 되었고, 해당 피처를 사용해 action classifier를 학습한 결과 정확도는 0.356을 기록하였다. 이는 오디오가 아예 없을 때의 정확도(0.177)보다 높으며, filtering 전 전체 생성 피처를 사용한 경우보다 noise가 줄어든 양상을 보였다. 향후 threshold 값을 조정하면서 precision-recall 균형을 최적화하는 추가 실험이 예정되어 있다. <br> <br> **[두번째 아키텍처 모듈]** <br> - **Representative Frame Extractor & CLIP-based Caption Generator**: 영상의 RGB frames에 대해 이동량이 큰 프레임 n개를 대표 이미지로 선정하고, CLIP 모델에 인풋으로 사용해 캡션을 생성 <br> - **Semantic Dictionary**: 학습 비디오의 대표 이미지 캡션과 AST 기반으로 추출된 오디오 라벨 간 의미적 관계를 BERT 임베딩을 통해 사전으로 구축 <br> - **Dictionary Referencing**: 생성된 캡션과 의미적으로 유사한 키 캡션을 사전에서 탐색, 해당 키 캡션과 연결된 오디오 라벨 임베딩을 참조 <br> - **Audio Feature Generator**: 랜덤 노이즈 + 딕셔너리에서 참조된 오디오 임베딩을 조건으로 하는 GAN 구조, 비디오 피처 시퀀스와 오디오 피처 시퀀스를 Dynamic Time Warping으로 정렬, 원본 오디오 피처를 ground truth로 하여 loss로 학습 |
| (4) 주요 기능의 구현 | **① Feature Filter(첫번째 아키텍처 모듈)** <br> - LSTM을 통해 생성된 오디오 피처는 AST 기반 Multi-label Predictor를 통해 예측된 오디오 라벨을 출력함 <br> - 해당 라벨은 BERT 임베딩된 Semantic Dictionary와 비교되어 의미 유사도(cosine similarity)가 계산됨 <br> - 이 때, Semantic Dictionary는 MiT action 라벨과 AudioSet 오디오 라벨의 BERT 기반 매핑으로 구축되어 있음 <br> - 예측된 오디오 라벨과 해당 비디오의 action label 간 의미 유사도가 **Threshold N** 이상인 경우에만 해당 오디오 피처를 정합한 것으로 간주 <br> 필터링된 오디오 피처만을 비디오 피처와 fuse하여 최종 classification에 사용해 정확도 향상 및 정합성 보장 <br> - Threshold N은 실험적으로 최적화되며, precision-recall tradeoff를 조절하는 핵심 하이퍼파라미터 <br> <br> **② Audio Feature Generator(두번째 아키텍처 모듈)** <br> - 대표 프레임에서 생성한 캡션과 Semantic Dictionary를 통해 의미적으로 매핑된 오디오 라벨 임베딩을 도출 <br> - 이를 조건으로 사용하여 랜덤 노이즈를 입력으로 하는 **Conditional GAN 구조**를 설계 <br> - Generator는 조건에 따라 의미 정합한 오디오 피처 시퀀스를 생성하며, Discriminator는 이를 평가 <br> - Dynamic Time Warping기법을 통해 비디오 피처 시퀀스와 생성된 오디오 피처 시퀀스 간 temporal alignment를 수행 <br> - Loss 구성: (1) Adversarial Loss (GAN 학습용), (2) Reconstruction Loss (원본 오디오 피처와의 유사도 기반) <br> - 이 복합 구조를 통해 비디오와 정렬되고 의미적으로 정합한 오디오 피처 시퀀스 생성 가능 |
| (5) 기타 | **[실험 핵심 패키지]** <br> `torch==1.13.1+cu116` / `torchaudio==0.13.1+cu116` / `torchvision==0.14.1+cu116` <br> → 모델 구현 및 학습에 사용 (GPU 가속 기반) <br> `transformers==4.49.0`, `tokenizers==0.21.0` <br> → ViT, BERT, CLIP, AST 등 사전 학습된 Transformer 모델 로딩 <br> `opencv-python==4.10.0.84`, `opencv-contrib-python==4.10.0.84` <br> → RGB/Optical Flow 이미지 처리 및 프레임 추출 <br> `numpy==1.23.5`, `pandas==2.2.3`, `scikit-learn==1.5.2`, `scipy==1.14.1` <br> → 데이터 처리, 벡터 계산, 통계 및 평가 지표 계산 <br> `soundfile==0.12.1`, `sox==1.5.0` <br> → 오디오 추출 및 스펙트로그램 변환 <br> `matplotlib==3.9.2`, `seaborn==0.13.2` <br> → 학습 결과 시각화 <br> `huggingface-hub==0.29.0`, `safetensors==0.5.2` <br> → 모델 다운로드 및 경량화 저장 지원 |

<br>
